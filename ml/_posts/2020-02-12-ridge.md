---
layout: post
title: "[지도 학습] Ridge / Lasso 회귀"
date: 2020-02-12
categories: [Machine Learning]
tag: [supervisedlearning,ridge,lasso]
---
## **[지도 학습] Ridge / Lasso 회귀**
{:.no_toc}

* 이 글은 [파이썬 라이브러리를 활용한 머신러닝](http://book.interpark.com/product/BookDisplay.do?_method=detail&sc.prdNo=303260973&gclid=CjwKCAiAyeTxBRBvEiwAuM8dnUQHS6gpLMB6tn0m3cKl_rO7hHyhwCzPtk23EvcW40nL99b1kC2ejBoCJKcQAvD_BwE)의 내용을 요약한 글입니다.
* 더불어 대학원 수업 "통계계산 방법론" 내용도 일부 포함하였습니다.

***

## **목차**
{:.no_toc}
0. this unordered seed list will be replaced by toc as unordered list
{:toc}

***

## **Python Libraries**
~~~python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston
~~~

***
## **Trade-off Between Variance and Bias**
* Ridge/ Lasso 회귀를 본격적으로 배우기 전 이해해야 할 개념은 분산과 편차의 Trade-off입니다. 왜냐하면 Ridge/ Lasso 추정치는 OLS (Ordinary Least Squares) 추정치 (선형 회귀 추정치)에 비해 더 편향되어 있지만 (biased), **분산이 낮아지기 때문에** 예측 측면에서 더 좋은 결과를 냅니다.
* 분산과 편차를 도식으로 표현하면 아래과 같습니다. 빨간색 원이 true 값의 영역이고 파란색 점이 예측값들이라 할 때 분산이 낮으면 예측값들이 응집되어 있고, 분산이 크면 예측값들이 퍼져 있습니다. 또한, 편차가 낮으면 예측값들이 빨간색 원을 중심으로 모여 있고, 높으면 빨간색 원에서 조금 벗어난 모습을 확인할 수 있습니다. 즉, 편차는 실제 값과 예측값들의 기댓값이 얼마나 차이가 나는 지를 나타냅니다.
![](../images/ridge-tradeoff.png)
* 고전적인 통계학의 관심사는 데이터에 맞는 모형을 만들고, 이 모형을 **해석 (Interpretation)** 하는 것이었다면, 최근 기계 학습에서 중시하는 관심사는 **예측 (Prediction)** 입니다. 즉, 새로운 $$\mathbf{X}_0$$ 데이터가 들어왔을 때 이 모형이 얼마나 새로운 값 $$\mathbf{y}_0$$ 정확하게 예측하는 지가 중요합니다. (여기서 $$\mathbf{X}$$를 대문자로 쓰는 이유는 $$p$$개의 설명 변수가 있을 때 $$n\times p$$형태의 행렬 $$\mathbf{X}$$로 설명변수를 표현하기 때문입니다) 이를 위해 여러 평가 척도 (RMSE, Accuracy, Sensitivity 등)이 개발되어 있습니다. 
* 만약 $$n$$개의 훈련 데이터를 $$\mathbf{T} = (\mathbf{X}_i, \mathbf{y}_i, i=1,\ldots,n)$$, 새로운 데이터셋을 (테스트 데이터)를 $$(\mathbf{X}_0, \mathbf{y}_0)$$라 할 때, 

$$
\mathbf{y} = f(\mathbf{X})+\varepsilon
$$

$$\mathbf{y}$$는 $$\mathbf{X}$$의 어떤 모르는 함수꼴로 표현된다 가정합시다 (단순 회귀라면 $$f(\mathbf{X}) = \mathbf{X} \boldsymbol{\beta}$$라 놓을 수 있습니다). 
여기서 우리가 최소화하고자 하는 손실 함수 (Loss function)는 예측 오차의 제곱 (Squared Prediction Error)을 최소화하는 것임으로 다음과 같이 정의됩니다.

$$
\begin{aligned}
&E\{(\mathbf{y}_0 - \widehat{f} (\mathbf{X}_0)\}^2\\
&= E\{ (\mathbf{y}_0 - f(\mathbf{X}_0)+f(\mathbf{X}_0) - \widehat{f}(\mathbf{X}_0))^2  \}\\
&= E\{ (\mathbf{y}_0 - f(\mathbf{X}_0))^2 + (f(\mathbf{X}_0) - \widehat{f}(\mathbf{X}_0))^2  \}\\
&= \underbrace{E\{(\mathbf{y}_0 - f(\mathbf{X}_0))^2\}}_{\text{Var}(\mathbf{y}_0)} + \underbrace{E\{(f(\mathbf{X}_0) - \widehat{f} (\mathbf{X}_0))^2\}}_{\text{Model Error}}
\end{aligned}
$$

  * 여기서 $$\text{Var}(\mathbf{y}_0)$$는 새로운 데이터 자체의 분산으로, 직접 통제할 수 없기 때문에 임의성 (Randomness)을 가진다 말합니다.
  * 따라서 우리는 그 나머지 부분인 Model Error를 최소화하는 것을 목표로 합니다. Model Error를 분해해보면, $$\widehat{f}(\mathbf{X}_0)$$의 분산 + 편차의 제곱 꼴로 나옴을 확인할 수 있습니다.

  $$
  \begin{aligned}
  &E\{(f(\mathbf{X}_0) - \widehat{f} (\mathbf{X}_0))^2\} \\
  &= E \left\{ \left(\widehat{f}(\mathbf{X}_0) -E(\widehat{f}(\mathbf{X}_0))+ E(\widehat{f}(\mathbf{X}_0))- f(\mathbf{X}_0)\right)^2\right\}\\
  &= \underbrace{E \left\{ \left(\widehat{f}(\mathbf{X}_0) -E(\widehat{f}(\mathbf{X}_0))\right)^2 \right\}}_{\text{Var}(\widehat{f}(\mathbf{X}_0))} +\underbrace{\left( E(\widehat{f}(\mathbf{X}_0)) - f(\mathbf{X}_0)\right)^2}_{\text{Bias}^2(\widehat{f}(\mathbf{X}_0))}
  \end{aligned}
  $$

  * 따라서 예측 오차의 제곱을 최소화하기 위해선 분산인 $$\text{Var}(\widehat{f}(\mathbf{X}_0))$$와  편차 $$\text{Bias}(f(\mathbf{X}_0))$$를 동시에 낮출수록 좋습니다.
  * 그러나, 분산과 편차를 동시에 낮추는 것은 불가능합니다. 모형이 복잡할수록 새로운 데이터의 적합 값에 대한 분산($$\text{Var}(\widehat{f}(\mathbf{X}_0))$$)은 커지고, 편차는 작아지는 성질을 가지고 있습니다. 이를 **분산과 편차의 Trade-off**라 부릅니다. 이를 표현하자면 다음과 같습니다. 
  ![](../images/ridge-tradeoff2.png) 
  * 모형이 복잡할수록 분산이 커지는 이유는 **과적합**(Overfitting) 때문입니다. 모형이 복잡하다는 뜻은 **훈련 데이터**의 특성에 딱 맞게 wiggly하게 적합되었음을 시사합니다. 아래의 그림과 같이 오른쪽으로 갈 수록 훈련데이터에 딱 맞게 적합되어 복잡성이 높습니다.
  ![](../images/ridge-complex.png)
  * 훈련 데이터에는 딱 맞지만, 새로운 $$\mathbf{X}_0$$가 들어오면 이 값에 따라 예측값 $$\mathbf{y}_0$$이 민감하게 바뀌고, 일반화가 되어있지 않기 때문에 예측값들의 분산이 큽니다. 
  * 모형이 복잡해질수록 편차는 작아집니다. 모형이 복잡해지면 모형의 space가 커져  예측값들의 평균과 실제값의 차이가 줄어듭니다.


***
## **Summary**
### 어떤 데이터에 Ridge/ Lasso 회귀를 사용할까?
1. **고차원 데이터**: 관측치의 개수 ($$n$$)와 설명변수의 개수(차원) $$p$$이 거의 같을 경우 모형이 복잡해져 분산이 커지고 편차는 작아집니다. 
 모형이 '복잡하다'는 것의 기준은 모수 공간 (parameter space)입니다. 설명 변수가 $$p$$개일 경우, 추정해야 하는 모수 $$\boldsymbol{\beta}$$의 공간은 $$\mathbb{R}^p$$차원이 됩니다. 심지어, $$n<p$$의 경우 선형 회귀를 통해 추정치를 내는 것이 불가능합니다. 
 이에 비해 Ridge/ Lasso 회귀는 모수에 제약을 주어 모형을 더 간단하게 만들어 예측치들의 분산을 낮춰 과적합을 방지합니다! 이렇게 복잡한 모형을 특정한 제약을 통해 더 간단하게 만드는 과정을 **정규화 (Regularization)** 라 부릅니다. 모형을 더 간단하게 한다는 말은 추정치 $$\boldsymbol{\beta}$$들을 0에 가깝게 만들어 설명 변수 $$\mathbf{X}$$가 반응 변수 $$\mathbf{y}$$에 주는 영향을 최소한으로 주는 것을 의미합니다.
 2. **다중공선성이 있는 데이터**: 1번과도 연관이 있는 말인데, 보통 고차원 데이터면 다중공선성이 있는 경우가 많습니다. **다중공선성**은 통계학에서도 매우 중요한 개념이고 기계 학습을 다룰 때에도 조심해야하는 부분인데요. 설명 변수들끼리 상관관계가 높으면 $$\mathbf{X}$$의 rank가 감소해 (=linearly independent, almost singular) OLS 추정치인 $$\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{Y}$$에서 역행렬 $$(\mathbf{X}^\top\mathbf{X})^{-1}$$를 구하기조차 불가능한 경우 다중공선성이 존재한다고 말합니다. 
 다중공선성 예시, 진단과 해결 방법은 [이 곳](https://datascienceschool.net/view-notebook/36176e580d124612a376cf29872cd2f0/)에서 자세히 볼 수 있습니다. 
 이 포스트에서 중요한 점은 Ridge/ Lasso 회귀가 다중공선성의 해결책이 될 수 있다는 점입니다. penalty term을 줌으로써 역행렬이 존재하도록 만들기 때문입니다.

### 선형 회귀에 비해 무엇이 나아졌을까?
 1. 더 간단한 모형으로 과적합 방지
 2. 분산 감소로 인한 더 좋은 예측력
 3. 고차원 데이터 적합 가능


***
## **Terminology**
* 과적합 (Overfitting): 훈련 데이터의 특성에 너무 가깝게 맞춰져서 (모형이 복잡해져서) 새로운 데이터에 일반화되기 어려운 경우 일어나는 현상
* 정규화 (Regularization): 
* 다중공선성 (Multicollinearity)

***
## **Ridge Regression**
*  

***
## **Reference**
* [Scott Fortmann-Roe의 블로그](http://scott.fortmann-roe.com/docs/BiasVariance.html#fn:4)
* https://www.mailman.columbia.edu/research/population-health-methods/ridge-regression
